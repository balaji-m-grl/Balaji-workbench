{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac19710d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created hierarchy: Synthetic Data in USB-PD_questions_hierarchy.md\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove excessive newlines and trim\"\"\"\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text.strip())\n",
    "    return text\n",
    "\n",
    "def extract_qa_and_followups(content: str) -> Tuple[List[Tuple[str, str, List[str]]], List[str]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        - List of (question, answer, [followup1, followup2, followup3])\n",
    "        - List of all main questions (for the flat questions file)\n",
    "    \"\"\"\n",
    "    # Normalize line endings\n",
    "    content = content.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    lines = content.split('\\n')\n",
    "\n",
    "    qa_pairs = []\n",
    "    all_main_questions = []\n",
    "    current_question = None\n",
    "    current_answer_lines = []\n",
    "    in_answer = False\n",
    "\n",
    "    # Patterns for detecting start of user question and start of ChatGPT response\n",
    "    user_markers = [\n",
    "        r'^(?:U|User|You):?\\s*(.+)$',\n",
    "        r'^Q:\\s*(.+)$',\n",
    "        r'^\\s*You\\s+asked:?\\s*(.+)$',\n",
    "    ]\n",
    "    assistant_markers = [\n",
    "        r'^(?:ChatGPT|Chat CPT|Assistant):?\\s*(.*)$',\n",
    "        r'^Answer:?\\s*(.*)$',\n",
    "        r'^\\[ChatGPT\\]\\s*(.*)$',\n",
    "    ]\n",
    "\n",
    "    # Follow-up patterns (very flexible)\n",
    "    followup_indicators = [\n",
    "        r'(?:here are|these are|some|following|recommended|next|related|follow.?up|you might also|want to ask|try asking|more questions).*?:\\s*$',\n",
    "        r'^(?:\\d[\\.\\)]\\s*|[\\-\\*]\\s*)(.+?)(?:\\?|\\.{3})?$',\n",
    "    ]\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "\n",
    "        # Detect start of new question\n",
    "        question_matched = False\n",
    "        for pattern in user_markers:\n",
    "            m = re.match(pattern, line, re.IGNORECASE)\n",
    "            if m:\n",
    "                if current_question and current_answer_lines:\n",
    "                    # Save previous Q&A\n",
    "                    answer = clean_text('\\n'.join(current_answer_lines))\n",
    "                    qa_pairs.append((current_question, answer, []))\n",
    "                    all_main_questions.append(current_question)\n",
    "\n",
    "                current_question = m.group(1).strip()\n",
    "                current_answer_lines = []\n",
    "                in_answer = False\n",
    "                question_matched = True\n",
    "                break\n",
    "\n",
    "        if question_matched:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Detect start of assistant response\n",
    "        for pattern in assistant_markers:\n",
    "            m = re.match(pattern, line, re.IGNORECASE)\n",
    "            if m and current_question:\n",
    "                in_answer = True\n",
    "                if m.group(1).strip():\n",
    "                    current_answer_lines.append(m.group(1).strip())\n",
    "                i += 1\n",
    "                break\n",
    "        else:\n",
    "            # Continuation line\n",
    "            if in_answer and line:\n",
    "                current_answer_lines.append(line)\n",
    "            i += 1\n",
    "\n",
    "    # Don't forget the last pair\n",
    "    if current_question and current_answer_lines:\n",
    "        answer = clean_text('\\n'.join(current_answer_lines))\n",
    "        qa_pairs.append((current_question, answer, []))\n",
    "\n",
    "    # Now extract follow-ups from each answer\n",
    "    for idx, (q, a, _) in enumerate(qa_pairs):\n",
    "        followups = []\n",
    "        lines_a = a.split('\\n')\n",
    "\n",
    "        collecting = False\n",
    "        for line in lines_a:\n",
    "            line_s = line.strip()\n",
    "\n",
    "            # Start collecting follow-ups?\n",
    "            if not collecting:\n",
    "                for pat in followup_indicators:\n",
    "                    if re.search(pat, line_s, re.IGNORECASE):\n",
    "                        collecting = True\n",
    "                        break\n",
    "                if collecting:\n",
    "                    continue\n",
    "\n",
    "            if collecting:\n",
    "                # Look for numbered or bulleted questions\n",
    "                m = re.match(r'^(?:\\d[\\.\\)]\\s*|[\\-\\*]\\s*)(.+?\\??)$', line_s)\n",
    "                if m:\n",
    "                    fu = m.group(1).strip()\n",
    "                    if fu and fu.endswith('?') or '...' in fu or len(fu) > 15:\n",
    "                        followups.append(fu)\n",
    "                elif line_s and not line_s.startswith('---') and not line_s.startswith('==='):\n",
    "                    # sometimes just plain lines after the header\n",
    "                    if '?' in line_s:\n",
    "                        followups.append(line_s)\n",
    "\n",
    "                # Stop collecting after ~4 lines or empty line + non-question\n",
    "                if len(followups) >= 3 or (not line_s and len(followups) > 0):\n",
    "                    break\n",
    "\n",
    "        # Keep only up to 3 good ones\n",
    "        followups = [f.strip() for f in followups if f.strip() and '?' in f][:3]\n",
    "        qa_pairs[idx] = (q, a, followups)\n",
    "\n",
    "    return qa_pairs, all_main_questions\n",
    "\n",
    "\n",
    "def process_file(input_path: str, output_dir: str):\n",
    "    \"\"\"Process one file and create all output markdown files\"\"\"\n",
    "    with open(input_path, encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    qa_pairs, main_questions = extract_qa_and_followups(content)\n",
    "\n",
    "    base_name = Path(input_path).stem\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Individual Q&A files\n",
    "    for i, (q, a, _) in enumerate(qa_pairs, 1):\n",
    "        filename = f\"{base_name}_Q{i}.md\"\n",
    "        path = os.path.join(output_dir, filename)\n",
    "\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"# Question {i}\\n\\n\")\n",
    "            f.write(f\"**Q:** {q}\\n\\n\")\n",
    "            f.write(f\"**A:**\\n\\n{a}\\n\")\n",
    "\n",
    "        print(f\"Created: {filename}\")\n",
    "\n",
    "    # 2. Hierarchy file\n",
    "    hierarchy_path = os.path.join(output_dir, f\"{base_name}_questions_hierarchy.md\")\n",
    "    with open(hierarchy_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"# Conversation Question Hierarchy\\n\\n\")\n",
    "\n",
    "        for i, (q, _, followups) in enumerate(qa_pairs, 1):\n",
    "            f.write(f\"## Q{i}\\n\")\n",
    "            f.write(f\"{q}\\n\\n\")\n",
    "\n",
    "            if followups:\n",
    "                f.write(\"**Follow-ups / Recommended:**\\n\\n\")\n",
    "                for j, fu in enumerate(followups, 1):\n",
    "                    f.write(f\"- **Q{i}.{j}**  {fu}\\n\")\n",
    "            f.write(\"\\n---\\n\\n\")\n",
    "\n",
    "        # Also plain list of all main questions\n",
    "        f.write(\"\\n## All Main Questions (flat list)\\n\\n\")\n",
    "        for i, q in enumerate(main_questions, 1):\n",
    "            f.write(f\"- Q{i}: {q}\\n\")\n",
    "\n",
    "    print(f\"Created hierarchy: {Path(hierarchy_path).name}\")\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Usage examples\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Change these paths\n",
    "    INPUT_FILE = \"C:\\\\Users\\\\GRL\\\\Downloads\\\\Synthetic Data in USB-PD.md\"\n",
    "#               ‚Üë just add r before the quote        # ‚Üê your file\n",
    "    OUTPUT_FOLDER = \"extracted_questions\"          # ‚Üê where to save\n",
    "\n",
    "    # Process one file\n",
    "    process_file(INPUT_FILE, OUTPUT_FOLDER)\n",
    "\n",
    "    # Or process many files in a folder\n",
    "    # input_folder = \"all_exports\"\n",
    "    # for file in Path(input_folder).glob(\"*.md\"):\n",
    "    #     process_file(str(file), OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec58fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd6d9d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n",
      "File loaded: 55,704 characters\n",
      "WARNING: No question-answer pairs were detected.\n",
      "First 800 characters of cleaned content:\n",
      "title: \"#Synthetic Data in USB-PD\"\n",
      "source: \"https://chatgpt.com/c/6971e805-20ec-8323-854a-caecf89489d2\"\n",
      "\n",
      "**You:**\n",
      "\n",
      "Wat is Synthetic data in model training,   \n",
      "Explain with USB pd domain agent model training example\n",
      "\n",
      "**You:**\n",
      "\n",
      "Wat is Synthetic data in model training,\n",
      "Explain with USB pd domain agent model training example\n",
      "\n",
      "**ChatGPT:**\n",
      "\n",
      "Synthetic data, in model training, is **artificially generated data that mimics real-world data**‚Äîits structure, patterns, edge cases, and constraints‚Äîwithout being directly collected from real systems or users.\n",
      "\n",
      "It‚Äôs not random data. Good synthetic data is **domain-faithful**, **statistically realistic**, and **behaviorally meaningful** for the task the model is learning.\n",
      "\n",
      "## Why synthetic data is used (quick context)\n",
      "\n",
      "In many engineering domains (USB-PD in\n",
      "--------------------------------------------------------------------------------\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove excessive newlines, junk separators, and trim\"\"\"\n",
    "    # Remove common junk separator lines\n",
    "    text = re.sub(r'^\\s*\\* \\* \\*\\s*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^\\s*[-*_]{3,}\\s*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text.strip())\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_followups(answer: str) -> List[str]:\n",
    "    \"\"\"Extract follow-up questions from typical ChatGPT endings\"\"\"\n",
    "    lines = answer.split('\\n')\n",
    "    followups = []\n",
    "    collecting = False\n",
    "\n",
    "    for line in lines:\n",
    "        s = line.strip()\n",
    "        lower = s.lower()\n",
    "\n",
    "        # Start collecting mode\n",
    "        if not collecting:\n",
    "            if any(p in lower for p in [\n",
    "                'if you want next', 'next we can', 'if you want, next i can',\n",
    "                'want next', 'next, i can', 'next steps', 'next question',\n",
    "                'next i can', 'shall we', 'my opinion', 'if you want'\n",
    "            ]):\n",
    "                collecting = True\n",
    "            continue\n",
    "\n",
    "        # Likely follow-up items\n",
    "        if s and (\n",
    "            s.startswith(('- ', '* ', '1. ', '2. ', '3. ', '‚Ä¢ ', '- **')) or\n",
    "            (s.endswith('?') and len(s) > 15) or\n",
    "            any(w in lower for w in ['design ', 'show ', 'map ', 'draft ', 'help '])\n",
    "        ):\n",
    "            cleaned = re.sub(r'^[-*‚Ä¢\\d.\\s*]+|\\s*\\**', '', s).strip()\n",
    "            if cleaned and cleaned not in followups:\n",
    "                followups.append(cleaned)\n",
    "\n",
    "        if len(followups) >= 3:\n",
    "            break\n",
    "\n",
    "    return followups[:3]\n",
    "\n",
    "\n",
    "def extract_qa_and_followups(content: str) -> Tuple[List[Tuple[str, str, List[str]]], List[str]]:\n",
    "    \"\"\"\n",
    "    Robust parser for this specific ChatGPT export style with junk lines\n",
    "    \"\"\"\n",
    "    content = clean_text(content.replace('\\r\\n', '\\n'))\n",
    "\n",
    "    # Split BEFORE each **You:** or **ChatGPT:** marker\n",
    "    parts = re.split(r'(?=\\n\\*\\*(?:You|ChatGPT)\\*\\*:)', '\\n' + content + '\\n')\n",
    "\n",
    "    qa_pairs = []\n",
    "    all_main_questions = []\n",
    "    current_question = None\n",
    "    current_answer_parts = []\n",
    "\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "\n",
    "        if part.startswith(\"**You**:\"):\n",
    "            # Save previous pair if exists\n",
    "            if current_question and current_answer_parts:\n",
    "                answer = clean_text('\\n'.join(current_answer_parts))\n",
    "                followups = extract_followups(answer)\n",
    "                qa_pairs.append((current_question, answer, followups))\n",
    "                all_main_questions.append(current_question)\n",
    "\n",
    "            # New question\n",
    "            question_text = part[8:].strip()\n",
    "            question_text = clean_text(question_text)\n",
    "\n",
    "            # Skip exact duplicate of previous question\n",
    "            if qa_pairs and question_text == qa_pairs[-1][0]:\n",
    "                current_question = None\n",
    "                current_answer_parts = []\n",
    "                continue\n",
    "\n",
    "            current_question = question_text\n",
    "            current_answer_parts = []\n",
    "\n",
    "        elif part.startswith(\"**ChatGPT**:\") and current_question:\n",
    "            # This is the answer (may contain multiple lines)\n",
    "            answer_content = part[12:].strip()\n",
    "            current_answer_parts.append(answer_content)\n",
    "\n",
    "    # Save last pair\n",
    "    if current_question and current_answer_parts:\n",
    "        answer = clean_text('\\n'.join(current_answer_parts))\n",
    "        followups = extract_followups(answer)\n",
    "        qa_pairs.append((current_question, answer, followups))\n",
    "        all_main_questions.append(current_question)\n",
    "\n",
    "    return qa_pairs, all_main_questions\n",
    "\n",
    "\n",
    "def process_file(input_path: str, output_dir: str):\n",
    "    \"\"\"Main processing function\"\"\"\n",
    "    try:\n",
    "        with open(input_path, encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"File loaded: {len(content):,} characters\")\n",
    "\n",
    "    qa_pairs, main_questions = extract_qa_and_followups(content)\n",
    "\n",
    "    if not qa_pairs:\n",
    "        print(\"WARNING: No question-answer pairs were detected.\")\n",
    "        print(\"First 800 characters of cleaned content:\")\n",
    "        cleaned = clean_text(content)\n",
    "        print(cleaned[:800])\n",
    "        print(\"-\" * 80)\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(qa_pairs)} question-answer pairs\")\n",
    "\n",
    "    base_name = Path(input_path).stem\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Individual Q&A files\n",
    "    for i, (q, a, followups) in enumerate(qa_pairs, 1):\n",
    "        filename = f\"{base_name}_Q{i}.md\"\n",
    "        path = os.path.join(output_dir, filename)\n",
    "\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"# Question {i}\\n\\n\")\n",
    "            f.write(f\"**Question:**\\n{q}\\n\\n\")\n",
    "            f.write(f\"**Answer:**\\n\\n{a}\\n\\n\")\n",
    "\n",
    "            if followups:\n",
    "                f.write(\"**Extracted follow-ups:**\\n\")\n",
    "                for j, fu in enumerate(followups, 1):\n",
    "                    f.write(f\"- {fu}\\n\")\n",
    "\n",
    "        print(f\"Created: {filename}\")\n",
    "\n",
    "    # Hierarchy file\n",
    "    hierarchy_path = os.path.join(output_dir, f\"{base_name}_hierarchy.md\")\n",
    "    with open(hierarchy_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"# Conversation Question Hierarchy\\n\\n\")\n",
    "\n",
    "        for i, (q, _, followups) in enumerate(qa_pairs, 1):\n",
    "            f.write(f\"## Q{i}\\n{q}\\n\\n\")\n",
    "            if followups:\n",
    "                f.write(\"**Follow-ups / Recommended:**\\n\")\n",
    "                for j, fu in enumerate(followups, 1):\n",
    "                    f.write(f\"- **Q{i}.{j}**  {fu}\\n\")\n",
    "            f.write(\"\\n---\\n\\n\")\n",
    "\n",
    "        f.write(\"## All Main Questions (flat)\\n\\n\")\n",
    "        for i, q in enumerate(main_questions, 1):\n",
    "            f.write(f\"- Q{i}: {q}\\n\")\n",
    "\n",
    "    print(f\"\\nHierarchy file: {Path(hierarchy_path).name}\")\n",
    "    print(\"Processing complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT_FILE = r\"C:\\Users\\GRL\\Downloads\\Synthetic Data in USB-PD.md\"\n",
    "    OUTPUT_FOLDER = \"extracted_usb_pd\"\n",
    "\n",
    "    print(\"Starting processing...\")\n",
    "    process_file(INPUT_FILE, OUTPUT_FOLDER)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1f460b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extraction...\n",
      "\n",
      "Reading file...\n",
      "File size: 55,704 characters\n",
      "Found 0 unique questions\n",
      "\n",
      "No questions found. Please check the file content.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_content(text: str) -> str:\n",
    "    \"\"\"Remove frontmatter, junk lines, normalize spacing\"\"\"\n",
    "    # Remove YAML frontmatter\n",
    "    text = re.sub(r'^---\\s*$.*?^---\\s*$', '', text, flags=re.MULTILINE | re.DOTALL)\n",
    "    \n",
    "    # Remove * * * and --- separator lines\n",
    "    text = re.sub(r'^\\s*\\* \\* \\*\\s*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^\\s*[-*_]{3,}\\s*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Normalize multiple newlines\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text.strip())\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_followups(answer_text: str) -> list:\n",
    "    \"\"\"Try to find follow-up / next-step questions in ChatGPT response\"\"\"\n",
    "    lines = answer_text.split('\\n')\n",
    "    followups = []\n",
    "    in_followup_section = False\n",
    "\n",
    "    for line in lines:\n",
    "        s = line.strip()\n",
    "        lower = s.lower()\n",
    "\n",
    "        if not in_followup_section:\n",
    "            if any(p in lower for p in [\n",
    "                'if you want next', 'next we can', 'if you want, next i can',\n",
    "                'want next', 'next, i can', 'next steps', 'next question',\n",
    "                'next i can', 'shall we', 'my opinion', 'if you want'\n",
    "            ]):\n",
    "                in_followup_section = True\n",
    "            continue\n",
    "\n",
    "        # Look for list items or question-like lines\n",
    "        if s and (\n",
    "            s.startswith(('- ', '* ', '1. ', '2. ', '3. ', '‚Ä¢ ')) or\n",
    "            (s.endswith('?') and len(s) > 15) or\n",
    "            any(kw in lower for kw in ['design', 'show', 'draft', 'map', 'help', 'convert'])\n",
    "        ):\n",
    "            cleaned = re.sub(r'^[-*‚Ä¢\\d.\\s]+|\\*\\*', '', s).strip()\n",
    "            if cleaned and cleaned not in followups:\n",
    "                followups.append(cleaned)\n",
    "\n",
    "        if len(followups) >= 3:\n",
    "            break\n",
    "\n",
    "    return followups[:3]\n",
    "\n",
    "\n",
    "def extract_conversation(filepath: str):\n",
    "    \"\"\"Extract questions and answers from the specific markdown format\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return []\n",
    "\n",
    "    print(\"Reading file...\")\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    print(f\"File size: {len(raw):,} characters\")\n",
    "\n",
    "    content = clean_content(raw)\n",
    "\n",
    "    # Split on **You:** lines (start of each question)\n",
    "    # We use positive lookbehind to keep the marker\n",
    "    parts = re.split(r'(?=\\*\\*You\\*\\*:)', content)\n",
    "\n",
    "    questions = []\n",
    "    current_answer = []\n",
    "\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "\n",
    "        if part.startswith(\"**You**:\"):\n",
    "            # If we have previous question + answer ‚Üí save it\n",
    "            if questions and current_answer:\n",
    "                prev_q, prev_a = questions[-1]\n",
    "                followups = extract_followups(prev_a)\n",
    "                questions[-1] = (prev_q, prev_a, followups)\n",
    "\n",
    "            # Start new question\n",
    "            q_text = part[8:].strip()  # remove **You**:\n",
    "            q_text = re.sub(r'\\s+', ' ', q_text).strip()\n",
    "            questions.append([q_text, \"\"])   # [question, answer]\n",
    "            current_answer = []\n",
    "\n",
    "        else:\n",
    "            # Everything else belongs to the previous answer\n",
    "            if questions:\n",
    "                current_answer.append(part)\n",
    "\n",
    "    # Save last answer\n",
    "    if questions and current_answer:\n",
    "        last_q, _ = questions[-1]\n",
    "        full_answer = '\\n\\n'.join(current_answer).strip()\n",
    "        followups = extract_followups(full_answer)\n",
    "        questions[-1] = (last_q, full_answer, followups)\n",
    "\n",
    "    # Remove duplicate questions (very common in your file)\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for q, a, f in questions:\n",
    "        if q not in seen:\n",
    "            seen.add(q)\n",
    "            unique.append((q, a, f))\n",
    "\n",
    "    print(f\"Found {len(unique)} unique questions\")\n",
    "    return unique\n",
    "\n",
    "\n",
    "def save_results(qa_list, input_path, output_dir):\n",
    "    base = Path(input_path).stem\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Individual files\n",
    "    for i, (q, a, followups) in enumerate(qa_list, 1):\n",
    "        fname = f\"{base}_Q{i}.md\"\n",
    "        path = os.path.join(output_dir, fname)\n",
    "\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"# Q{i}\\n\\n\")\n",
    "            f.write(f\"**Question**\\n\\n{q}\\n\\n\")\n",
    "            f.write(f\"**Answer**\\n\\n{a}\\n\\n\")\n",
    "            if followups:\n",
    "                f.write(\"**Follow-ups / Next steps:**\\n\")\n",
    "                for j, fu in enumerate(followups, 1):\n",
    "                    f.write(f\"- {fu}\\n\")\n",
    "\n",
    "        print(f\"Created: {fname}\")\n",
    "\n",
    "    # Hierarchy overview\n",
    "    hierarchy_path = os.path.join(output_dir, f\"{base}_hierarchy.md\")\n",
    "    with open(hierarchy_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"# Conversation Hierarchy\\n\\n\")\n",
    "\n",
    "        for i, (q, _, followups) in enumerate(qa_list, 1):\n",
    "            f.write(f\"## Q{i}\\n\")\n",
    "            f.write(f\"{q}\\n\\n\")\n",
    "            if followups:\n",
    "                f.write(\"**Follow-ups / Recommended:**\\n\")\n",
    "                for j, fu in enumerate(followups, 1):\n",
    "                    f.write(f\"- **Q{i}.{j}**  {fu}\\n\")\n",
    "            f.write(\"\\n---\\n\\n\")\n",
    "\n",
    "        f.write(\"## All questions (flat list)\\n\\n\")\n",
    "        for i, (q, _, _) in enumerate(qa_list, 1):\n",
    "            f.write(f\"- Q{i}: {q}\\n\")\n",
    "\n",
    "    print(f\"\\nHierarchy file: {Path(hierarchy_path).name}\")\n",
    "    print(\"All files saved.\")\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "#  MAIN\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT_FILE = r\"C:\\Users\\GRL\\Downloads\\Synthetic Data in USB-PD.md\"\n",
    "    OUTPUT_DIR = \"extracted_questions\"\n",
    "\n",
    "    print(\"Starting extraction...\\n\")\n",
    "    qa_pairs = extract_conversation(INPUT_FILE)\n",
    "\n",
    "    if qa_pairs:\n",
    "        print(\"\\nSaving results...\")\n",
    "        save_results(qa_pairs, INPUT_FILE, OUTPUT_DIR)\n",
    "        print(\"\\nDone.\")\n",
    "    else:\n",
    "        print(\"\\nNo questions found. Please check the file content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ad0a0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: C:\\Users\\GRL\\Downloads\\Synthetic Data in USB-PD.md\n",
      "‚ö†Ô∏è  No Q&A pairs detected.\n",
      "First 600 characters of file:\n",
      "----------------------------------------------------------------------\n",
      "---\n",
      "title: \"#Synthetic Data in USB-PD\"\n",
      "source: \"https://chatgpt.com/c/6971e805-20ec-8323-854a-caecf89489d2\"\n",
      "---\n",
      "\n",
      "**You:**\n",
      "\n",
      "Wat is Synthetic data in model training,   \n",
      "Explain with USB pd domain agent model training example\n",
      "\n",
      "* * *\n",
      "\n",
      "**You:**\n",
      "\n",
      "Wat is Synthetic data in model training,\n",
      "Explain with USB pd domain agent model training example\n",
      "\n",
      "* * *\n",
      "\n",
      "**ChatGPT:**\n",
      "\n",
      "Synthetic data, in model training, is **artificially generated data that mimics real-world data**‚Äîits structure, patterns, edge cases, and constraints‚Äîwithout being directly collected from real systems or users.\n",
      "\n",
      "It‚Äôs not random data. Good \n",
      "----------------------------------------------------------------------\n",
      "Check if the role markers are different than expected.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# More robust patterns ‚Äì matches your exact file format\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "USER_PATTERNS = [\n",
    "    r\"^\\*\\*You:\\*\\*\",           # **You:**\n",
    "    r\"^\\*\\*You:\\s\",             # **You:** (with space after :)\n",
    "    r\"^\\*\\*User:\\*\\*\",\n",
    "    r\"^\\*\\*Human:\\*\\*\",\n",
    "]\n",
    "\n",
    "ASSISTANT_PATTERNS = [\n",
    "    r\"^\\*\\*ChatGPT:\\*\\*\",\n",
    "    r\"^\\*\\*ChatGPT:\\s\",         # **ChatGPT:** (with space)\n",
    "    r\"^\\*\\*Assistant:\\*\\*\",\n",
    "    r\"^\\*\\*Grok:\\*\\*\",\n",
    "]\n",
    "\n",
    "USER_RE = re.compile(\"|\".join(USER_PATTERNS), re.IGNORECASE)\n",
    "ASSISTANT_RE = re.compile(\"|\".join(ASSISTANT_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "SEPARATOR_RE = re.compile(r\"^(\\* \\* \\*|---)$\", re.IGNORECASE)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Split conversation into (user, assistant) pairs\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def split_conversation(md_text: str) -> List[Tuple[str, str]]:\n",
    "    lines = md_text.splitlines()\n",
    "\n",
    "    pairs = []\n",
    "    current_role = None\n",
    "    current_user = []\n",
    "    current_assistant = []\n",
    "\n",
    "    def flush():\n",
    "        if current_user or current_assistant:\n",
    "            user_text = \"\\n\".join(current_user).strip()\n",
    "            assistant_text = \"\\n\".join(current_assistant).strip()\n",
    "            # Only add if there's meaningful content\n",
    "            if user_text and (assistant_text or len(current_assistant) > 0):\n",
    "                pairs.append((user_text, assistant_text))\n",
    "\n",
    "    i = 0\n",
    "    # Skip YAML frontmatter\n",
    "    if lines and lines[0].strip() == '---':\n",
    "        i = 1\n",
    "        while i < len(lines) and not lines[i].strip() == '---':\n",
    "            i += 1\n",
    "        i += 1  # skip closing ---\n",
    "\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        stripped = line.strip()\n",
    "\n",
    "        if USER_RE.match(line):\n",
    "            flush()\n",
    "            current_role = \"user\"\n",
    "            current_user = []\n",
    "            current_assistant = []\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        if ASSISTANT_RE.match(line):\n",
    "            flush()\n",
    "            current_role = \"assistant\"\n",
    "            current_user = []\n",
    "            current_assistant = []\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        if SEPARATOR_RE.match(stripped):\n",
    "            flush()\n",
    "            current_role = None\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Add line to current block (but skip the role marker lines themselves)\n",
    "        if current_role == \"user\":\n",
    "            current_user.append(line)\n",
    "        elif current_role == \"assistant\":\n",
    "            current_assistant.append(line)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    flush()\n",
    "    return pairs\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Extract suggested / follow-up questions\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def extract_suggested_questions(text: str) -> List[str]:\n",
    "    if not text.strip():\n",
    "        return []\n",
    "\n",
    "    suggestions = set()\n",
    "\n",
    "    lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
    "\n",
    "    # Bullet points and numbered lists\n",
    "    for line in lines:\n",
    "        if re.match(r\"^[-*‚Ä¢]|\\d+[.)]\\s+\", line):\n",
    "            clean = re.sub(r\"^[-*‚Ä¢]|\\d+[.)]\\s*\", \"\", line).strip()\n",
    "            if len(clean) > 6:\n",
    "                suggestions.add(clean)\n",
    "\n",
    "    # Sentences ending with ?\n",
    "    for line in lines:\n",
    "        if line.endswith(\"?\") and len(line) > 15:\n",
    "            suggestions.add(line)\n",
    "\n",
    "    # Lines starting with common suggestion verbs / phrases\n",
    "    suggestion_starts = re.compile(\n",
    "        r\"^(next|try|you can|consider|also|would you like|want to|how about|what if|can you|should|let\\'?s|another|more|extend|improve|optimize|debug|test|analyze|design|show|explain|add)\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    for line in lines:\n",
    "        if suggestion_starts.match(line) and len(line) > 20:\n",
    "            suggestions.add(line)\n",
    "\n",
    "    # After headers like \"If you want, next we can:\"\n",
    "    capture = False\n",
    "    for line in lines:\n",
    "        lower = line.lower()\n",
    "        if \"if you want\" in lower or \"next we can\" in lower or \"next:\" in lower:\n",
    "            capture = True\n",
    "            continue\n",
    "        if capture and (line.startswith(\"- \") or line.startswith(\"* \") or re.match(r\"^\\d+[.)] \", line)):\n",
    "            clean = re.sub(r\"^[-*‚Ä¢]|\\d+[.)]\\s*\", \"\", line).strip()\n",
    "            if clean:\n",
    "                suggestions.add(clean)\n",
    "        if capture and len(line.strip()) > 0 and not line.strip().startswith(('-','*','1.','2.','3.')):\n",
    "            capture = False\n",
    "\n",
    "    # Sort by length (prefer longer/more specific ones first)\n",
    "    return sorted(list(suggestions), key=len, reverse=True)[:15]\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Write outputs\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def write_outputs(pairs: List[Tuple[str, str]], output_dir: Path):\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # questions_only.md ‚îÄ‚îÄ only questions + AI suggestions\n",
    "    q_only = output_dir / \"questions_only.md\"\n",
    "    with q_only.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"# Questions + AI Suggested Follow-ups\\n\\n\")\n",
    "\n",
    "        for i, (question, answer) in enumerate(pairs, 1):\n",
    "            if not question.strip():\n",
    "                continue\n",
    "\n",
    "            f.write(f\"## Q{i}\\n\\n\")\n",
    "            f.write(question.strip() + \"\\n\\n\")\n",
    "\n",
    "            suggestions = extract_suggested_questions(answer)\n",
    "            if suggestions:\n",
    "                f.write(\"**AI suggested follow-ups:**\\n\\n\")\n",
    "                for j, sug in enumerate(suggestions, 1):\n",
    "                    f.write(f\"- Q{i}.{j}  {sug}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "                f.write(\"_No clear follow-up suggestions detected_\\n\\n\")\n",
    "\n",
    "    # Individual QA files\n",
    "    for i, (q, a) in enumerate(pairs, 1):\n",
    "        if not q.strip():\n",
    "            continue\n",
    "\n",
    "        filepath = output_dir / f\"Q{i:03d}.md\"\n",
    "        with filepath.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"# Question {i}\\n\\n\")\n",
    "            f.write(\"## User\\n\\n\" + q.strip() + \"\\n\\n\")\n",
    "            f.write(\"## ChatGPT\\n\\n\")\n",
    "            if a.strip():\n",
    "                f.write(a.strip() + \"\\n\\n\")\n",
    "            else:\n",
    "                f.write(\"_No answer captured_\\n\\n\")\n",
    "\n",
    "            suggestions = extract_suggested_questions(a)\n",
    "            if suggestions:\n",
    "                f.write(\"## AI Suggested Follow-ups\\n\\n\")\n",
    "                for j, sug in enumerate(suggestions, 1):\n",
    "                    f.write(f\"- Q{i}.{j}  {sug}\\n\")\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Main\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def main():\n",
    "    path_str = input(\"Enter path to the .md conversation file: \").strip().strip('\"')\n",
    "    file_path = Path(path_str)\n",
    "\n",
    "    if not file_path.is_file():\n",
    "        print(\"‚ùå File not found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Reading: {file_path}\")\n",
    "    text = file_path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    pairs = split_conversation(text)\n",
    "\n",
    "    if not pairs:\n",
    "        print(\"‚ö†Ô∏è  No Q&A pairs detected.\")\n",
    "        print(\"First 600 characters of file:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(text[:600])\n",
    "        print(\"-\" * 70)\n",
    "        print(\"Check if the role markers are different than expected.\")\n",
    "        return\n",
    "\n",
    "    output_dir = file_path.parent / \"extracted_questions\"\n",
    "    write_outputs(pairs, output_dir)\n",
    "\n",
    "    print(\"\\n\" + \"‚ïê\" * 70)\n",
    "    print(f\"Success! Found {len(pairs)} question-answer pairs\")\n",
    "    print(f\"Output folder: {output_dir.resolve()}\")\n",
    "    print(\"‚Üí questions_only.md contains questions + follow-up suggestions only\")\n",
    "    print(\"‚ïê\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from typing import List, Tuple, Dict\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- Ollama Config ----------\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"qwen2.5:3b\"  # fast & light\n",
    "MAX_CHARS = 2000  # limit text sent to model\n",
    "\n",
    "# ---------- Role markers ----------\n",
    "USER_PATTERNS = [\n",
    "    r\"^\\*\\*You:\\*\\*\",\n",
    "    r\"^\\*\\*User:\\*\\*\",\n",
    "    r\"^User:\",\n",
    "    r\"^You:\",\n",
    "    r\"^##\\s*User\",\n",
    "    r\"^##\\s*You\",\n",
    "]\n",
    "\n",
    "ASSISTANT_PATTERNS = [\n",
    "    r\"^\\*\\*ChatGPT:\\*\\*\",\n",
    "    r\"^\\*\\*Assistant:\\*\\*\",\n",
    "    r\"^Assistant:\",\n",
    "    r\"^ChatGPT:\",\n",
    "    r\"^##\\s*Assistant\",\n",
    "    r\"^##\\s*ChatGPT\",\n",
    "]\n",
    "\n",
    "USER_RE = re.compile(\"|\".join(USER_PATTERNS), re.IGNORECASE)\n",
    "ASSISTANT_RE = re.compile(\"|\".join(ASSISTANT_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "# ---------- Split conversation ----------\n",
    "\n",
    "def split_conversation(md_text: str) -> List[Tuple[str, str]]:\n",
    "    lines = md_text.splitlines()\n",
    "\n",
    "    blocks = []\n",
    "    current_role = None\n",
    "    current_user = []\n",
    "    current_assistant = []\n",
    "\n",
    "    def flush():\n",
    "        if current_user or current_assistant:\n",
    "            blocks.append((\n",
    "                \"\\n\".join(current_user).strip(),\n",
    "                \"\\n\".join(current_assistant).strip()\n",
    "            ))\n",
    "\n",
    "    for line in lines:\n",
    "        s = line.strip()\n",
    "\n",
    "        if USER_RE.match(s):\n",
    "            if current_role == \"assistant\":\n",
    "                flush()\n",
    "                current_user = []\n",
    "                current_assistant = []\n",
    "            current_role = \"user\"\n",
    "            continue\n",
    "\n",
    "        if ASSISTANT_RE.match(s):\n",
    "            current_role = \"assistant\"\n",
    "            continue\n",
    "\n",
    "        if current_role == \"user\":\n",
    "            current_user.append(line)\n",
    "        elif current_role == \"assistant\":\n",
    "            current_assistant.append(line)\n",
    "\n",
    "    flush()\n",
    "    return [(u, a) for u, a in blocks if u.strip() or a.strip()]\n",
    "\n",
    "# ---------- AI Extraction (with caching) ----------\n",
    "\n",
    "def extract_suggestions_with_ai(answer_text: str, cache: Dict[str, List[str]]) -> List[str]:\n",
    "    key = answer_text.strip()\n",
    "    if not key:\n",
    "        return []\n",
    "\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "\n",
    "    text = key\n",
    "    if len(text) > MAX_CHARS:\n",
    "        text = text[:MAX_CHARS]\n",
    "\n",
    "    prompt = (\n",
    "        \"Extract all follow-up or suggested questions from the text below.\\n\"\n",
    "        \"Return ONLY a JSON array of strings.\\n\"\n",
    "        \"If none, return [].\\n\\n\"\n",
    "        \"Text:\\n\"\n",
    "        '\"\"\"\\n'\n",
    "        f\"{text}\\n\"\n",
    "        '\"\"\"\\n'\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(OLLAMA_URL, json=payload, timeout=300)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        out = data.get(\"response\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Ollama request failed:\", e)\n",
    "        cache[key] = []\n",
    "        return []\n",
    "\n",
    "    # Try to extract JSON array safely\n",
    "    suggestions: List[str] = []\n",
    "    try:\n",
    "        start = out.find(\"[\")\n",
    "        end = out.rfind(\"]\") + 1\n",
    "        if start != -1 and end != -1:\n",
    "            arr = json.loads(out[start:end])\n",
    "            if isinstance(arr, list):\n",
    "                suggestions = [s.strip() for s in arr if isinstance(s, str) and s.strip()]\n",
    "    except Exception:\n",
    "        suggestions = []\n",
    "\n",
    "    cache[key] = suggestions\n",
    "    return suggestions\n",
    "\n",
    "# ---------- Write outputs ----------\n",
    "\n",
    "def write_outputs(pairs: List[Tuple[str, str]], output_dir: Path):\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cache: Dict[str, List[str]] = {}\n",
    "\n",
    "    # Precompute suggestions\n",
    "    all_suggestions = []\n",
    "    for q, a in pairs:\n",
    "        sugg = extract_suggestions_with_ai(a, cache)\n",
    "        all_suggestions.append(sugg)\n",
    "\n",
    "    # questions_only.md\n",
    "    questions_only = output_dir / \"questions_only.md\"\n",
    "    with questions_only.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"# Suggested / Follow-up Questions (AI Extracted)\\n\\n\")\n",
    "\n",
    "        for i, ((q, a), suggestions) in enumerate(zip(pairs, all_suggestions), 1):\n",
    "            if not suggestions:\n",
    "                continue\n",
    "\n",
    "            f.write(f\"## Q{i}\\n\\n\")\n",
    "            f.write(q.strip() + \"\\n\\n\")\n",
    "\n",
    "            for j, s in enumerate(suggestions, 1):\n",
    "                f.write(f\"- Q{i}.{j} {s}\\n\")\n",
    "\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    # Individual Q&A files\n",
    "    for i, ((q, a), suggestions) in enumerate(zip(pairs, all_suggestions), 1):\n",
    "        out = output_dir / f\"Q{i:03d}.md\"\n",
    "        with out.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"# Question {i}\\n\\n\")\n",
    "            f.write(\"## User Question\\n\\n\")\n",
    "            f.write(q.strip() + \"\\n\\n\")\n",
    "            f.write(\"## Assistant Answer\\n\\n\")\n",
    "            f.write(a.strip() + \"\\n\\n\")\n",
    "\n",
    "            if suggestions:\n",
    "                f.write(\"## Suggested / Follow-up Questions\\n\\n\")\n",
    "                for j, s in enumerate(suggestions, 1):\n",
    "                    f.write(f\"- Q{i}.{j} {s}\\n\")\n",
    "\n",
    "# ---------- Main ----------\n",
    "\n",
    "def main():\n",
    "    input_path = input(\"Enter path to conversation .md file: \").strip().strip('\"')\n",
    "    file_path = Path(input_path)\n",
    "\n",
    "    if not file_path.exists():\n",
    "        print(\"‚ùå File not found!\")\n",
    "        return\n",
    "\n",
    "    text = file_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    pairs = split_conversation(text)\n",
    "\n",
    "    if not pairs:\n",
    "        print(\"‚ö†Ô∏è No conversation blocks detected.\")\n",
    "        return\n",
    "\n",
    "    base_dir = Path(r\"D:\\Balaji-workbench\\synthetic data\")\n",
    "    run_name = datetime.now().strftime(\"output_%Y%m%d_%H%M%S\")\n",
    "    output_dir = base_dir / run_name\n",
    "\n",
    "    write_outputs(pairs, output_dir)\n",
    "\n",
    "    print(\"‚úÖ Done!\")\n",
    "    print(f\"üìÑ Processed {len(pairs)} Q&A pairs\")\n",
    "    print(f\"üìÅ Output folder: {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
